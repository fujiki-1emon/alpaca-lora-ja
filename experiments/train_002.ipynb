{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57833a26",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/aws-ofi-nccl/lib:/usr/local/lib:/usr/lib: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//127.0.0.1'), PosixPath('56529'), PosixPath('pmix-server.2040307;tcp4')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('/usr/share/modules/$MODULE_VERSION/modulefiles')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/modules/$MODULE_VERSION/modulefiles')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n eval `eval ${_mlre}/usr/bin/tclsh8.6 /usr/lib/x86_64-linux-gnu/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh8.6 /usr/lib/x86_64-linux-gnu/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "2023-05-13 09:59:22.979366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-13 09:59:31.386973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers as T\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162dd3b4",
   "metadata": {},
   "source": [
    "## 2023-05-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02b5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES = \"/fsx/jp-llm/instruction_tuning/datasets/japanese_alpaca_data.json\"\n",
    "VAL_SET_SIZE = 2000\n",
    "CUTOFF_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7431e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/admin/home-fujiki/.cache/huggingface/datasets/json/default-a28b5bb2c6979ed7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ddbb1417e545ef915608a1d13d5033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /admin/home-fujiki/.cache/huggingface/datasets/json/default-a28b5bb2c6979ed7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-547705db6822a05d.arrow and /admin/home-fujiki/.cache/huggingface/datasets/json/default-a28b5bb2c6979ed7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3c493914ff88bf93.arrow\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=DATA_FILES)\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4886a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "        # sorry about the formatting disaster gotta move fast\n",
    "        if data_point[\"input\"]:\n",
    "            return f\"\"\"以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
    "### 命令:\n",
    "{data_point[\"instruction\"]}\n",
    "### 入力:\n",
    "{data_point[\"input\"]}\n",
    "### 応答:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
    "### 命令:\n",
    "{data_point[\"instruction\"]}\n",
    "### 応答:\n",
    "{data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381ce953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '「関節炎の人にとって、最適な運動はヨガ、水泳、またはウォーキングなどの低負荷の運動です。これらの運動は、関節炎の症状を悪化させることなく、運動の利点を提供します。」',\n",
       " 'instruction': '「関節炎を持つ人にとって最適な運動は何ですか？」',\n",
       " 'input': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a8b465",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e29419bd43b4e4798045ffccb2d3b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2988452/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2741065921.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/2741065921.py'</span>               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">578</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 575 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 576 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>: <span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span> = kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"self\"</span>)                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 577 │   │   # apply actual function</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 578 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out: Union[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"DatasetDict\"</span>] = func(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 579 │   │   </span>datasets: List[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(out.values()) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(out, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">el</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 580 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> dataset <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> datasets:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 581 │   │   │   # Remove task templates if a column mapping of the template is no long</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">543</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 540 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"output_all_columns\"</span>: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._output_all_columns,                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 541 │   │   </span>}                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 542 │   │   # apply actual function</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out: Union[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"DatasetDict\"</span>] = func(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 544 │   │   </span>datasets: List[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(out.values()) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(out, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">el</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 545 │   │   # re-apply format to the output</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 546 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> dataset <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> datasets:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">307</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">map</span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3070 │   │   │   │   │   </span>leave=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3071 │   │   │   │   │   </span>desc=desc <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"Map\"</span>,                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3072 │   │   │   │   </span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> pbar:                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3073 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> rank, done, content <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> Dataset._map_single(**dataset_kwarg <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3074 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> done:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3075 │   │   │   │   │   │   │   </span>shards_done += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3076 │   │   │   │   │   │   │   </span>logger.debug(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Finished processing shard number {</span>rank<span style=\"color: #808000; text-decoration-color: #808000\">}</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">342</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_map_single</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3424 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> batched:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3425 │   │   │   │   │   </span>_time = time.time()                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3426 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, example <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> shard_iterable:                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3427 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>example = apply_function_on_filtered_inputs(example, i, of <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3428 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> update_data:                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3429 │   │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> i == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3430 │   │   │   │   │   │   │   │   </span>buf_writer, writer, tmp_file = init_buffer_and_wri <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">334</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_function_on_filtered_inputs</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3338 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> update_data <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3339 │   │   │   │   # Check if the function returns updated examples</span>                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3340 │   │   │   │   </span>update_data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, (Mapping, pa.Table))    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3341 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>validate_function_output(processed_inputs, indices)                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3342 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> update_data:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3343 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Nothing to update, let's move on</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3344 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> shard._format_type <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> input_columns:                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">328</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">validate_function_output</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3283 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">validate_function_output</span>(processed_inputs, indices):                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3284 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Validate output of the map function.\"\"\"</span>                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3285 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> processed_inputs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, ( <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3286 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>(                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3287 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Provided `function` which is applied to all elements of tabl</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3288 │   │   │   │   </span>)                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3289 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(indices, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, Mappin <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>Provided `function` which is applied to all elements of table returns a variable \n",
       "of type <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>. Make sure provided `function` returns a variable of type `dict` <span style=\"font-weight: bold\">(</span>or a \n",
       "pyarrow table<span style=\"font-weight: bold\">)</span> to update the dataset or `<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>` if you are only interested in side effects.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2988452/\u001b[0m\u001b[1;33m2741065921.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/2741065921.py'\u001b[0m               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m578\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m in \u001b[92mwrapper\u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 575 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 576 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m: \u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mself\u001b[0m\u001b[33m\"\u001b[0m)                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 577 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# apply actual function\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 578 \u001b[2m│   │   \u001b[0mout: Union[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mDatasetDict\u001b[0m\u001b[33m\"\u001b[0m] = func(\u001b[96mself\u001b[0m, *args, **kwargs)         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 579 \u001b[0m\u001b[2m│   │   \u001b[0mdatasets: List[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[96mlist\u001b[0m(out.values()) \u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(out, \u001b[96mdict\u001b[0m) \u001b[94mel\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 580 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m dataset \u001b[95min\u001b[0m datasets:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 581 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Remove task templates if a column mapping of the template is no long\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m543\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m in \u001b[92mwrapper\u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 540 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33moutput_all_columns\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m._output_all_columns,                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 541 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 542 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# apply actual function\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 543 \u001b[2m│   │   \u001b[0mout: Union[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mDatasetDict\u001b[0m\u001b[33m\"\u001b[0m] = func(\u001b[96mself\u001b[0m, *args, **kwargs)         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 544 \u001b[0m\u001b[2m│   │   \u001b[0mdatasets: List[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[96mlist\u001b[0m(out.values()) \u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(out, \u001b[96mdict\u001b[0m) \u001b[94mel\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 545 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# re-apply format to the output\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 546 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m dataset \u001b[95min\u001b[0m datasets:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m307\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m3\u001b[0m in \u001b[92mmap\u001b[0m                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3070 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mleave=\u001b[94mFalse\u001b[0m,                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3071 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdesc=desc \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mMap\u001b[0m\u001b[33m\"\u001b[0m,                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3072 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m) \u001b[94mas\u001b[0m pbar:                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3073 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m rank, done, content \u001b[95min\u001b[0m Dataset._map_single(**dataset_kwarg \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3074 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m done:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3075 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mshards_done += \u001b[94m1\u001b[0m                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3076 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mlogger.debug(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFinished processing shard number \u001b[0m\u001b[33m{\u001b[0mrank\u001b[33m}\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m342\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m7\u001b[0m in \u001b[92m_map_single\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3424 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m batched:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3425 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m_time = time.time()                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3426 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m i, example \u001b[95min\u001b[0m shard_iterable:                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3427 \u001b[2m│   │   │   │   │   │   \u001b[0mexample = apply_function_on_filtered_inputs(example, i, of \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3428 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m update_data:                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3429 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m i == \u001b[94m0\u001b[0m:                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3430 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   \u001b[0mbuf_writer, writer, tmp_file = init_buffer_and_wri \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m334\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mapply_function_on_filtered_inputs\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3338 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m update_data \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3339 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Check if the function returns updated examples\u001b[0m                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3340 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mupdate_data = \u001b[96misinstance\u001b[0m(processed_inputs, (Mapping, pa.Table))    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3341 \u001b[2m│   │   │   │   \u001b[0mvalidate_function_output(processed_inputs, indices)                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3342 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m update_data:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3343 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mNone\u001b[0m  \u001b[2m# Nothing to update, let's move on\u001b[0m                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3344 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m shard._format_type \u001b[95mor\u001b[0m input_columns:                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m328\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m6\u001b[0m in \u001b[92mvalidate_function_output\u001b[0m                                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3283 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mvalidate_function_output\u001b[0m(processed_inputs, indices):                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3284 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\"\"Validate output of the map function.\"\"\"\u001b[0m                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3285 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m processed_inputs \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(processed_inputs, ( \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3286 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3287 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mProvided `function` which is applied to all elements of tabl\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3288 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3289 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(indices, \u001b[96mlist\u001b[0m) \u001b[95mand\u001b[0m \u001b[96misinstance\u001b[0m(processed_inputs, Mappin \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mProvided `function` which is applied to all elements of table returns a variable \n",
       "of type \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m. Make sure provided `function` returns a variable of type `dict` \u001b[1m(\u001b[0mor a \n",
       "pyarrow table\u001b[1m)\u001b[0m to update the dataset or `\u001b[3;35mNone\u001b[0m` if you are only interested in side effects.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data_prompt = val_data.map(lambda x: generate_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2082ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\\n### 命令:\\n「関節炎を持つ人にとって最適な運動は何ですか？」\\n### 応答:\\n「関節炎の人にとって、最適な運動はヨガ、水泳、またはウォーキングなどの低負荷の運動です。これらの運動は、関節炎の症状を悪化させることなく、運動の利点を提供します。」'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1c14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "### 命令:\n",
      "「関節炎を持つ人にとって最適な運動は何ですか？」\n",
      "### 応答:\n",
      "「関節炎の人にとって、最適な運動はヨガ、水泳、またはウォーキングなどの低負荷の運動です。これらの運動は、関節炎の症状を悪化させることなく、運動の利点を提供します。」\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(val_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d9baa4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e29419bd43b4e4798045ffccb2d3b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2988452/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2741065921.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/2741065921.py'</span>               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">578</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 575 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 576 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>: <span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span> = kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"self\"</span>)                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 577 │   │   # apply actual function</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 578 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out: Union[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"DatasetDict\"</span>] = func(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 579 │   │   </span>datasets: List[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(out.values()) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(out, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">el</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 580 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> dataset <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> datasets:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 581 │   │   │   # Remove task templates if a column mapping of the template is no long</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">543</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 540 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"output_all_columns\"</span>: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._output_all_columns,                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 541 │   │   </span>}                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 542 │   │   # apply actual function</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out: Union[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"DatasetDict\"</span>] = func(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 544 │   │   </span>datasets: List[<span style=\"color: #808000; text-decoration-color: #808000\">\"Dataset\"</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(out.values()) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(out, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">el</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 545 │   │   # re-apply format to the output</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 546 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> dataset <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> datasets:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">307</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">map</span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3070 │   │   │   │   │   </span>leave=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3071 │   │   │   │   │   </span>desc=desc <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"Map\"</span>,                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3072 │   │   │   │   </span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> pbar:                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3073 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> rank, done, content <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> Dataset._map_single(**dataset_kwarg <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3074 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> done:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3075 │   │   │   │   │   │   │   </span>shards_done += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3076 │   │   │   │   │   │   │   </span>logger.debug(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Finished processing shard number {</span>rank<span style=\"color: #808000; text-decoration-color: #808000\">}</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">342</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_map_single</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3424 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> batched:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3425 │   │   │   │   │   </span>_time = time.time()                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3426 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, example <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> shard_iterable:                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3427 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>example = apply_function_on_filtered_inputs(example, i, of <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3428 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> update_data:                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3429 │   │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> i == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3430 │   │   │   │   │   │   │   │   </span>buf_writer, writer, tmp_file = init_buffer_and_wri <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">334</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_function_on_filtered_inputs</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3338 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> update_data <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3339 │   │   │   │   # Check if the function returns updated examples</span>                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3340 │   │   │   │   </span>update_data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, (Mapping, pa.Table))    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3341 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>validate_function_output(processed_inputs, indices)                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3342 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> update_data:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3343 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Nothing to update, let's move on</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3344 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> shard._format_type <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> input_columns:                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">arrow_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">328</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">validate_function_output</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3283 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">validate_function_output</span>(processed_inputs, indices):                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3284 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Validate output of the map function.\"\"\"</span>                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3285 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> processed_inputs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, ( <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3286 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>(                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3287 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Provided `function` which is applied to all elements of tabl</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3288 │   │   │   │   </span>)                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3289 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(indices, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(processed_inputs, Mappin <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>Provided `function` which is applied to all elements of table returns a variable \n",
       "of type <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>. Make sure provided `function` returns a variable of type `dict` <span style=\"font-weight: bold\">(</span>or a \n",
       "pyarrow table<span style=\"font-weight: bold\">)</span> to update the dataset or `<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>` if you are only interested in side effects.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2988452/\u001b[0m\u001b[1;33m2741065921.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/2741065921.py'\u001b[0m               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m578\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m in \u001b[92mwrapper\u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 575 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 576 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m: \u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mself\u001b[0m\u001b[33m\"\u001b[0m)                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 577 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# apply actual function\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 578 \u001b[2m│   │   \u001b[0mout: Union[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mDatasetDict\u001b[0m\u001b[33m\"\u001b[0m] = func(\u001b[96mself\u001b[0m, *args, **kwargs)         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 579 \u001b[0m\u001b[2m│   │   \u001b[0mdatasets: List[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[96mlist\u001b[0m(out.values()) \u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(out, \u001b[96mdict\u001b[0m) \u001b[94mel\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 580 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m dataset \u001b[95min\u001b[0m datasets:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 581 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Remove task templates if a column mapping of the template is no long\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m543\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m in \u001b[92mwrapper\u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 540 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33moutput_all_columns\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m._output_all_columns,                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 541 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 542 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# apply actual function\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 543 \u001b[2m│   │   \u001b[0mout: Union[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mDatasetDict\u001b[0m\u001b[33m\"\u001b[0m] = func(\u001b[96mself\u001b[0m, *args, **kwargs)         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 544 \u001b[0m\u001b[2m│   │   \u001b[0mdatasets: List[\u001b[33m\"\u001b[0m\u001b[33mDataset\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[96mlist\u001b[0m(out.values()) \u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(out, \u001b[96mdict\u001b[0m) \u001b[94mel\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 545 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# re-apply format to the output\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 546 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m dataset \u001b[95min\u001b[0m datasets:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m307\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m3\u001b[0m in \u001b[92mmap\u001b[0m                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3070 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mleave=\u001b[94mFalse\u001b[0m,                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3071 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdesc=desc \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mMap\u001b[0m\u001b[33m\"\u001b[0m,                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3072 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m) \u001b[94mas\u001b[0m pbar:                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3073 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m rank, done, content \u001b[95min\u001b[0m Dataset._map_single(**dataset_kwarg \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3074 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m done:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3075 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mshards_done += \u001b[94m1\u001b[0m                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3076 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mlogger.debug(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFinished processing shard number \u001b[0m\u001b[33m{\u001b[0mrank\u001b[33m}\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m342\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m7\u001b[0m in \u001b[92m_map_single\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3424 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m batched:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3425 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m_time = time.time()                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3426 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m i, example \u001b[95min\u001b[0m shard_iterable:                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3427 \u001b[2m│   │   │   │   │   │   \u001b[0mexample = apply_function_on_filtered_inputs(example, i, of \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3428 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m update_data:                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3429 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m i == \u001b[94m0\u001b[0m:                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3430 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   \u001b[0mbuf_writer, writer, tmp_file = init_buffer_and_wri \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m334\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mapply_function_on_filtered_inputs\u001b[0m                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3338 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m update_data \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3339 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Check if the function returns updated examples\u001b[0m                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3340 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mupdate_data = \u001b[96misinstance\u001b[0m(processed_inputs, (Mapping, pa.Table))    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3341 \u001b[2m│   │   │   │   \u001b[0mvalidate_function_output(processed_inputs, indices)                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3342 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m update_data:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3343 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mNone\u001b[0m  \u001b[2m# Nothing to update, let's move on\u001b[0m                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3344 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m shard._format_type \u001b[95mor\u001b[0m input_columns:                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/fsx/home-fujiki/venvs/polyglot/lib/python3.8/site-packages/datasets/\u001b[0m\u001b[1;33marrow_dataset.py\u001b[0m:\u001b[94m328\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[94m6\u001b[0m in \u001b[92mvalidate_function_output\u001b[0m                                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3283 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mvalidate_function_output\u001b[0m(processed_inputs, indices):                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3284 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\"\"Validate output of the map function.\"\"\"\u001b[0m                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3285 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m processed_inputs \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(processed_inputs, ( \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3286 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3287 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mProvided `function` which is applied to all elements of tabl\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3288 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3289 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(indices, \u001b[96mlist\u001b[0m) \u001b[95mand\u001b[0m \u001b[96misinstance\u001b[0m(processed_inputs, Mappin \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mProvided `function` which is applied to all elements of table returns a variable \n",
       "of type \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m. Make sure provided `function` returns a variable of type `dict` \u001b[1m(\u001b[0mor a \n",
       "pyarrow table\u001b[1m)\u001b[0m to update the dataset or `\u001b[3;35mNone\u001b[0m` if you are only interested in side effects.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data_token = val_data.map(lambda x: tokenize(generate_prompt(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c8c080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2988452/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">363386410.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/363386410.py'</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2988452/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4250418532.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">tokenize</span>                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/4250418532.py'</span>               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'tokenizer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2988452/\u001b[0m\u001b[1;33m363386410.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/363386410.py'\u001b[0m                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2988452/\u001b[0m\u001b[1;33m4250418532.py\u001b[0m:\u001b[94m4\u001b[0m in \u001b[92mtokenize\u001b[0m                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2988452/4250418532.py'\u001b[0m               \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'tokenizer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize(generate_prompt(val_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96916f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False)  # , add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c7847b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3174, 4771, 18507, 1499, 36631, 3], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f16d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23006, 36372, 18356, 36384, 13283, 4187, 36379, 36372, 14571, 164, 5822, 16170, 36503, 2526, 4830, 16, 36375, 3765, 36384, 16560, 3535, 28480, 16869, 12575, 7317, 36375, 5670, 37700, 37700, 36370, 4187, 36489, 87, 7864, 37732, 871, 26484, 9036, 1172, 5604, 1499, 1123, 5670, 37700, 37700, 36370, 16869, 36489, 87, 7864, 37732, 319, 898, 36372, 9036, 28270, 9247, 36372, 18136, 36372, 908, 15580, 309, 37119, 9965, 36371, 1172, 16, 36375, 2962, 28270, 36372, 7864, 16128, 28668, 8405, 647, 5890, 36372, 25333, 36637, 2431, 692, 97, 1405, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(generate_prompt(val_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef5f7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False, add_eos_token=True)\n",
    "\n",
    "def tokenize(prompt):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = _tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"][:-1],\n",
    "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f693a95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23006,\n",
       "  36372,\n",
       "  18356,\n",
       "  36384,\n",
       "  13283,\n",
       "  4187,\n",
       "  36379,\n",
       "  36372,\n",
       "  14571,\n",
       "  164,\n",
       "  5822,\n",
       "  16170,\n",
       "  36503,\n",
       "  2526,\n",
       "  4830,\n",
       "  16,\n",
       "  36375,\n",
       "  3765,\n",
       "  36384,\n",
       "  16560,\n",
       "  3535,\n",
       "  28480,\n",
       "  16869,\n",
       "  12575,\n",
       "  7317,\n",
       "  36375,\n",
       "  5670,\n",
       "  37700,\n",
       "  37700,\n",
       "  36370,\n",
       "  4187,\n",
       "  36489,\n",
       "  87,\n",
       "  7864,\n",
       "  37732,\n",
       "  871,\n",
       "  26484,\n",
       "  9036,\n",
       "  1172,\n",
       "  5604,\n",
       "  1499,\n",
       "  1123,\n",
       "  5670,\n",
       "  37700,\n",
       "  37700,\n",
       "  36370,\n",
       "  16869,\n",
       "  36489,\n",
       "  87,\n",
       "  7864,\n",
       "  37732,\n",
       "  319,\n",
       "  898,\n",
       "  36372,\n",
       "  9036,\n",
       "  28270,\n",
       "  9247,\n",
       "  36372,\n",
       "  18136,\n",
       "  36372,\n",
       "  908,\n",
       "  15580,\n",
       "  309,\n",
       "  37119,\n",
       "  9965,\n",
       "  36371,\n",
       "  1172,\n",
       "  16,\n",
       "  36375,\n",
       "  2962,\n",
       "  28270,\n",
       "  36372,\n",
       "  7864,\n",
       "  16128,\n",
       "  28668,\n",
       "  8405,\n",
       "  647,\n",
       "  5890,\n",
       "  36372,\n",
       "  25333,\n",
       "  36637,\n",
       "  2431,\n",
       "  692,\n",
       "  97,\n",
       "  1405,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(generate_prompt(val_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a5d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b27454b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67261d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_BATCH_SIZE = 4\n",
    "BATCH_SIZE = 128\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 3 \n",
    "LEARNING_RATE = 3e-4\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "VAL_SET_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a188f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aceea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79711af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False, add_eos_token=True)\n",
    "    # config = AutoConfig.from_pretrained(model_dir)\n",
    "    # config.is_decoder = True\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"rinna/japanese-gpt-1b\",\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.to(\"cuda:0\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7acff85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"rinna/japanese-gpt-1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c71c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    \"/fsx/jp-llm/instruction_tuning/outputs/train_002_rinna-1b_ja-alpaca-data/lora\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a39ac8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_prompt(instruction, _input=None):\n",
    "    if _input:\n",
    "        return f\"\"\"以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
    "\n",
    "### 命令:\n",
    "{instruction}\n",
    "\n",
    "### 入力:\n",
    "{_input}\n",
    "\n",
    "### 応答:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
    "\n",
    "### 入力:\n",
    "{instruction}\n",
    "\n",
    "### 応答:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eeee7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e1c2693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "gen_output = evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81489181",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    # temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df079019",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=384\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a2db148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s>タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなる\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c271396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6829de90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ee90502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', '[PAD]')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0), tokenizer.decode(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66a12503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85d8b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d79bf993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9445f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fcb51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False)\n",
    "model_cudstom = AutoModelForCausalLM.from_pretrained(\n",
    "    \"rinna/japanese-gpt-1b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_original = AutoModelForCausalLM.from_pretrained(\n",
    "    \"rinna/japanese-gpt-1b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_custom = PeftModel.from_pretrained(\n",
    "    model_custom, \n",
    "    \"/fsx/jp-llm/instruction_tuning/outputs/train_002_rinna-1b_ja-alpaca-data/lora\",\n",
    ")\n",
    "# model_original = PeftModel.from_pretrained(\n",
    "#     model_original, \n",
    "#     \"rinna/japanese-gpt-1b\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74a9d1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "2 <s>\n",
      "3 </s>\n",
      "4 [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.cls_token_id, tokenizer.decode(tokenizer.cls_token_id))\n",
    "print(tokenizer.bos_token_id, tokenizer.decode(tokenizer.bos_token_id))\n",
    "print(tokenizer.eos_token_id, tokenizer.decode(tokenizer.eos_token_id))\n",
    "print(tokenizer.pad_token_id, tokenizer.decode(tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6481aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44960ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b18f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_instruction_prompt(\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4cc62e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23006, 36372, 18356, 36384, 13283, 4187, 36379, 36372, 14571, 164, 5822, 16170, 36503, 2526, 4830, 16, 36375, 3765, 36384, 16560, 3535, 28480, 16869, 12575, 7317, 36375, 5670, 37700, 37700, 36370, 3498, 36489, 3174, 4771, 18507, 1499, 36631, 5670, 37700, 37700, 36370, 16869, 36489, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15addcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b52f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d7b7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=16,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8dc2b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=0.75,\n",
    "    num_beams=16,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48e0a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s>タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。 ### タスクを説明する命令と、さらなる\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ac4b7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=384,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8a7cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    # top_p=0.75,\n",
    "    num_beams=4,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "153bbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    # top_p=0.75,\n",
    "    num_beams=16,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a576e25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  以下は、タスクを説明する命令と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たすような応答を書きなさい。\n",
      "\n",
      "### 入力:\n",
      "日本の首都はどこですか？\n",
      "\n",
      "### 応答:\n",
      "-----\n",
      "input_ids.shape = torch.Size([1, 44])\n",
      "---- Response -----\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "model_custom.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    # top_p=0.75,\n",
    "    num_beams=128,\n",
    ")\n",
    "def evaluate(model_aaa, instruction, input=None):\n",
    "    prompt = generate_instruction_prompt(instruction, input)\n",
    "    print(f\"Prompt: \", prompt)\n",
    "    print(\"-----\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    print(f\"input_ids.shape = {input_ids.shape}\")\n",
    "    generation_output = model_aaa.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0])\n",
    "    # print(\"Output: \", output)\n",
    "    # print(\"-----\")\n",
    "    response = output.split(\"### 応答:\")[1].strip()\n",
    "    if \"命令\" in response:\n",
    "        response = response.split(\"### 命令\")[0].strip()\n",
    "    print(\"---- Response -----\")\n",
    "    print(response)\n",
    "\n",
    "evaluate(model_custom,\"日本の首都はどこですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2913337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
